name: Deploy to Azure AKS

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "production"
        type: choice
        options:
          - production

concurrency:
  group: deploy-aks-${{ github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: cloudtolocalllm
  API_IMAGE: cloudtolocalllm/cloudtolocalllm-api
  WEB_IMAGE: cloudtolocalllm/cloudtolocalllm-web
  STREAMING_PROXY_IMAGE: cloudtolocalllm/cloudtolocalllm-streaming-proxy
  POSTGRES_IMAGE: cloudtolocalllm/cloudtolocalllm-postgres
  AZURE_RESOURCE_GROUP: cloudtolocalllm-rg
  AZURE_CLUSTER_NAME: cloudtolocalllm-aks
  AZURE_CONTAINER_REGISTRY: cloudtolocalllm

jobs:
  build:
    # This job builds and pushes the Docker images.
    runs-on: ubuntu-latest
    outputs:
      api-image: ${{ steps.meta-api.outputs.tags }}
      web-image: ${{ steps.meta-web.outputs.tags }}
      postgres-image: ${{ steps.meta-postgres.outputs.tags }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract API metadata
        id: meta-api
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.API_IMAGE }}
          tags: |
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: ./services/api-backend
          file: ./services/api-backend/Dockerfile.prod
          push: true
          tags: ${{ steps.meta-api.outputs.tags }}
          labels: ${{ steps.meta-api.outputs.labels }}

      - name: Extract Web metadata
        id: meta-web
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.WEB_IMAGE }}
          tags: |
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build Web image (test locally first)
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./config/docker/Dockerfile.web
          push: false
          load: true
          tags: web-test:latest
          cache-from: type=gha
          build-args: |
            BUILD_SHA=${{ github.sha }}

      - name: Test Web image
        run: |
          echo "Testing web image..."

          # Start container
          docker run -d --name web-test -p 8080:8080 web-test:latest
          sleep 3

          # Test health endpoint
          echo "Testing health endpoint..."
          if curl -f http://localhost:8080/health; then
            echo "✅ Health endpoint working"
          else
            echo "❌ Health endpoint failed"
            docker logs web-test
            docker stop web-test
            exit 1
          fi

          # Test index.html
          echo "Testing index.html..."
          if curl -f http://localhost:8080/index.html | grep -q "flutter"; then
            echo "✅ index.html contains Flutter content"
          else
            echo "❌ index.html missing or invalid"
            curl http://localhost:8080/index.html | head -20
            docker logs web-test
            docker stop web-test
            exit 1
          fi

          # Test main.dart.js
          echo "Testing main.dart.js..."
          if curl -f http://localhost:8080/main.dart.js > /dev/null 2>&1; then
            SIZE=$(curl -s http://localhost:8080/main.dart.js | wc -c)
            echo "✅ main.dart.js exists (size: $SIZE bytes)"
          else
            echo "❌ main.dart.js not found"
            docker logs web-test
            docker stop web-test
            exit 1
          fi

          docker stop web-test
          echo "✅ Web image test passed"

      - name: Build and push Web image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./config/docker/Dockerfile.web
          push: true
          tags: ${{ steps.meta-web.outputs.tags }}
          labels: ${{ steps.meta-web.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_SHA=${{ github.sha }}

      - name: Extract Streaming Proxy metadata
        id: meta-streaming-proxy
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.STREAMING_PROXY_IMAGE }}
          tags: |
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Streaming Proxy image
        uses: docker/build-push-action@v5
        with:
          context: ./services/streaming-proxy
          file: ./services/streaming-proxy/Dockerfile
          push: true
          tags: ${{ steps.meta-streaming-proxy.outputs.tags }}
          labels: ${{ steps.meta-streaming-proxy.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Extract Postgres metadata
        id: meta-postgres
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.POSTGRES_IMAGE }}
          tags: |
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Postgres image
        uses: docker/build-push-action@v5
        with:
          context: ./config/docker
          file: ./config/docker/Dockerfile.postgres
          push: true
          tags: ${{ steps.meta-postgres.outputs.tags }}
          labels: ${{ steps.meta-postgres.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment: production
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify Docker images were built
        run: |
          echo "Verifying Docker images..."
          echo "API Image: ${{ env.API_IMAGE }}:latest"
          echo "Web Image: ${{ env.WEB_IMAGE }}:latest"
          echo "Postgres Image: ${{ env.POSTGRES_IMAGE }}:latest"
          echo "✅ Docker images ready for deployment"

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Get AKS credentials
        run: |
          az aks get-credentials \
            --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
            --name ${{ env.AZURE_CLUSTER_NAME }}

      - name: Create or update Kubernetes secret
        run: |
          # Ensure namespace exists
          kubectl apply -f k8s/namespace.yaml

          # Apply ConfigMap
          kubectl apply -f k8s/configmap.yaml

          kubectl create secret generic cloudtolocalllm-secrets \
            --from-literal=postgres-user='cloud_admin' \
            --from-literal=postgres-password='CloudToLocalSecurePass2024!' \
            --from-literal=jwt-secret='${{ secrets.JWT_SECRET }}' \
            --from-literal=auth0-domain='${{ secrets.AUTH0_DOMAIN }}' \
            --from-literal=auth0-audience='${{ secrets.AUTH0_AUDIENCE }}' \
            --from-literal=auth0-client-id='${{ secrets.AUTH0_CLIENT_ID }}' \
            --from-literal=stripe-test-secret-key='${{ secrets.STRIPE_TEST_SECRET_KEY }}' \
            --from-literal=stripe-test-publishable-key='${{ secrets.STRIPE_TEST_PUBLISHABLE_KEY }}' \
            --from-literal=stripe-test-webhook-secret='${{ secrets.STRIPE_TEST_WEBHOOK_SECRET }}' \
            --from-literal=sentry-dsn='${{ secrets.SENTRY_DSN }}' \
            -n cloudtolocalllm --dry-run=client -o yaml | kubectl apply -f -

      - name: Update API deployment
        run: |
          echo "Applying API deployment manifest..."
          kubectl apply -f k8s/api-backend-deployment.yaml

          echo "Updating API deployment with image: ${{ env.API_IMAGE }}:latest"
          kubectl set image deployment/api-backend \
            api-backend=${{ env.API_IMAGE }}:latest \
            -n cloudtolocalllm

          # Force restart to ensure new image is pulled
          kubectl rollout restart deployment/api-backend -n cloudtolocalllm
          echo "API deployment updated successfully"

      - name: Update Web deployment
        run: |
          echo "Applying Web deployment manifest..."
          kubectl apply -f k8s/web-deployment.yaml

          echo "Updating web deployment with image: ${{ env.WEB_IMAGE }}:latest"
          kubectl set image deployment/web \
            web=${{ env.WEB_IMAGE }}:latest \
            -n cloudtolocalllm
          echo "Web deployment updated successfully"

      - name: Update Postgres deployment
        run: |
          echo "Applying Postgres StatefulSet manifest..."
          kubectl apply -f k8s/postgres-statefulset.yaml

          echo "Updating Postgres deployment with image: ${{ env.POSTGRES_IMAGE }}:latest"
          kubectl set image statefulset/postgres \
            postgres=${{ env.POSTGRES_IMAGE }}:latest \
            -n cloudtolocalllm

          # Force restart to ensure new image is pulled
          kubectl rollout restart statefulset/postgres -n cloudtolocalllm
          echo "Postgres deployment updated successfully"

      - name: Apply streaming-proxy resources
        continue-on-error: false
        run: |
          echo "Deploying streaming-proxy resources..."

          # Create streaming-proxy secrets
          kubectl create secret generic streaming-proxy-secrets \
            --from-literal=SENTRY_DSN='${{ secrets.SENTRY_DSN }}' \
            -n cloudtolocalllm --dry-run=client -o yaml | kubectl apply -f -

          # Apply ConfigMap and other resources
          kubectl apply -f k8s/streaming-proxy-configmap.yaml
          kubectl apply -f k8s/streaming-proxy-service.yaml
          kubectl apply -f k8s/streaming-proxy-deployment.yaml

          echo "✅ Streaming-proxy resources deployed"

      - name: Ensure cert-manager is installed
        continue-on-error: true
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "Checking if cert-manager is installed..."
          if kubectl get namespace cert-manager 2>/dev/null; then
            echo "✅ cert-manager namespace exists"
          else
            echo "Installing cert-manager..."
            kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml
            echo "Waiting for cert-manager to be ready..."
            kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=300s
          fi

          # Create Cloudflare API token secret for cert-manager
          echo "Creating Cloudflare API token secret..."
          if [ -z "$CLOUDFLARE_API_TOKEN" ]; then
            echo "⚠️ CLOUDFLARE_API_TOKEN not set in secrets"
            echo "Please set CLOUDFLARE_API_TOKEN in GitHub secrets with DNS:Edit permissions"
            exit 1
          fi

          kubectl create secret generic cloudflare-api-token \
            --from-literal=api-token="$CLOUDFLARE_API_TOKEN" \
            -n cert-manager \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "✅ Cloudflare API token secret created"

          # Apply cert-manager ClusterIssuers with Cloudflare DNS-01 challenge
          echo "Applying cert-manager ClusterIssuer configuration (staging and production)..."
          kubectl apply -f k8s/cert-manager-cloudflare-dns.yaml

          echo "✅ cert-manager configured for automatic SSL certificate provisioning using Cloudflare DNS-01 challenge"
          echo "   - Staging issuer: cloudflare-dns-staging (for testing)"
          echo "   - Production issuer: cloudflare-dns-prod (ready for production)"

          # Apply wildcard certificate resource
          echo "Applying wildcard certificate configuration..."
          kubectl apply -f k8s/certificate-wildcard.yaml

          echo "✅ Wildcard certificate (*.cloudtolocalllm.online) will be automatically provisioned via Cloudflare DNS"
          echo "   Certificate will be issued by Let's Encrypt using Cloudflare DNS-01 challenge"

      - name: Apply Ingress Configuration
        run: |
          echo "Applying Ingress configuration..."
          kubectl apply -f k8s/ingress-nginx.yaml
          echo "✅ Ingress applied successfully"

      - name: Configure Cloudflare SSL (Full Strict)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "Configuring Cloudflare SSL mode to Full (Strict)..."

          if [ -z "$CLOUDFLARE_API_TOKEN" ]; then
            echo "❌ Cloudflare API token not set in secrets"
            exit 1
          fi

          CF_API_TOKEN="$CLOUDFLARE_API_TOKEN"

          # Get Zone ID
          CF_ZONE_ID=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones?name=cloudtolocalllm.online" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" | jq -r '.result[0].id')

          if [ "$CF_ZONE_ID" = "null" ] || [ -z "$CF_ZONE_ID" ]; then
            echo "❌ Unable to determine Cloudflare Zone ID"
            exit 1
          fi

          echo "Found Zone ID: $CF_ZONE_ID"

          # Update SSL setting to 'full' (Full)
          # We use 'full' instead of 'strict' to allow self-signed certificates during provisioning
          # This prevents 526 errors if the Let's Encrypt certificate is renewing or not yet ready
          # Traffic is still encrypted end-to-end
          RESPONSE=$(curl -s -X PATCH "https://api.cloudflare.com/client/v4/zones/$CF_ZONE_ID/settings/ssl" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data '{"value": "full"}')

          SUCCESS=$(echo "$RESPONSE" | jq -r '.success')
          if [ "$SUCCESS" = "true" ]; then
            echo "✅ Cloudflare SSL mode set to 'full'"
            echo "   - Traffic between Cloudflare and AKS is encrypted"
            echo "   - Allows self-signed certificates (prevents 526 errors during provisioning)"
          else
            echo "❌ Failed to change SSL mode"
            echo "$RESPONSE" | jq '.errors'
            exit 1
          fi

      - name: Wait for rollout to complete
        continue-on-error: true
        run: |
          echo "Waiting for deployments to rollout..."

          echo "Checking API backend deployment..."
          kubectl rollout status deployment/api-backend -n cloudtolocalllm --timeout=300s || {
            echo "⚠️ API backend rollout timed out - checking pod status"
            kubectl describe deployment api-backend -n cloudtolocalllm
            kubectl get pods -n cloudtolocalllm -l app=api-backend
          }

          echo ""
          echo "Checking web deployment..."
          kubectl rollout status deployment/web -n cloudtolocalllm --timeout=300s || {
            echo "⚠️ Web deployment rollout timed out - checking pod status"
            kubectl describe deployment web -n cloudtolocalllm
            kubectl get pods -n cloudtolocalllm -l app=web
          }

          echo ""
          echo "Deployment rollout check completed"

      - name: Check Pod Health (Smart Restart Detection)
        run: |
          echo "Checking for pod health and stability..."

          # Get pod status and restart information
          echo "API Backend pods status:"
          kubectl get pods -n cloudtolocalllm -l app=api-backend -o wide

          echo ""
          echo "Analyzing pod health..."

          # Get detailed pod information as JSON
          POD_INFO=$(kubectl get pods -n cloudtolocalllm -l app=api-backend -o json)

          # Count total pods
          TOTAL_PODS=$(echo "$POD_INFO" | jq '.items | length')
          echo "Total API backend pods: $TOTAL_PODS"

          # Count running/ready pods
          RUNNING_PODS=$(echo "$POD_INFO" | jq '[.items[] | select(.status.phase == "Running" and (.status.conditions[] | select(.type == "Ready" and .status == "True")))] | length')
          echo "Running/ready pods: $RUNNING_PODS"

          # Count terminating pods (normal during rolling updates)
          TERMINATING_PODS=$(echo "$POD_INFO" | jq '[.items[] | select(.metadata.deletionTimestamp != null)] | length')
          echo "Terminating pods: $TERMINATING_PODS"

          # Calculate restarts only for non-terminating pods
          ACTIVE_RESTARTS=$(echo "$POD_INFO" | jq '[.items[] | select(.metadata.deletionTimestamp == null) | .status.containerStatuses[0].restartCount // 0] | add')
          echo "Restarts in active pods: $ACTIVE_RESTARTS"

          # Check if deployment is progressing normally
          if [ "$RUNNING_PODS" -gt 0 ] && [ "$ACTIVE_RESTARTS" -eq 0 ]; then
            echo "✅ Deployment healthy: $RUNNING_PODS pods running, $TERMINATING_PODS terminating, 0 restarts in active pods"
            exit 0
          fi

          # Check for unhealthy pods (restarting active pods)
          if [ "$ACTIVE_RESTARTS" -gt 0 ]; then
            echo "❌ Detected $ACTIVE_RESTARTS restarts in active pods - investigating..."

            # Show detailed pod status
            echo ""
            echo "Detailed pod analysis:"
            echo "$POD_INFO" | jq -r '.items[] | "Pod: \(.metadata.name) | Phase: \(.status.phase) | Restarts: \(.status.containerStatuses[0].restartCount // 0) | Terminating: \(.metadata.deletionTimestamp != null)"'

            echo ""
            echo "--- Current Logs from active pods ---"
            kubectl get pods -n cloudtolocalllm -l app=api-backend -o json | jq -r '.items[] | select(.metadata.deletionTimestamp == null) | .metadata.name' | head -1 | xargs -I {} kubectl logs {} -n cloudtolocalllm --tail=30 --all-containers=true || true

            echo ""
            echo "--- Previous Logs from active pods ---"
            kubectl get pods -n cloudtolocalllm -l app=api-backend -o json | jq -r '.items[] | select(.metadata.deletionTimestamp == null) | .metadata.name' | head -1 | xargs -I {} kubectl logs {} -n cloudtolocalllm --tail=30 --previous --all-containers=true || true

            echo ""
            echo "❌ Failing deployment due to restarts in active pods"
            exit 1
          fi

          # Allow deployment if old pods are terminating but new ones are healthy
          if [ "$TERMINATING_PODS" -gt 0 ] && [ "$RUNNING_PODS" -gt 0 ] && [ "$ACTIVE_RESTARTS" -eq 0 ]; then
            echo "✅ Rolling update in progress: $TERMINATING_PODS terminating, $RUNNING_PODS healthy, 0 restarts in active pods"
            exit 0
          fi

          # Edge case: no running pods but some pods exist
          if [ "$RUNNING_PODS" -eq 0 ] && [ "$TOTAL_PODS" -gt 0 ]; then
            echo "⚠️ No healthy pods found - checking status..."
            kubectl describe pods -n cloudtolocalllm -l app=api-backend
            echo "❌ No healthy pods - deployment may be failing"
            exit 1
          fi

          echo "✅ Pod health check passed"

      - name: Verify deployment
        run: |
          echo "::group::Deployment Verification"

          echo "Pods in cloudtolocalllm namespace:"
          kubectl get pods -n cloudtolocalllm -o wide

          echo ""
          echo "Services in cloudtolocalllm namespace:"
          kubectl get services -n cloudtolocalllm

          echo ""
          echo "Ingress in cloudtolocalllm namespace:"
          kubectl get ingress -n cloudtolocalllm

          echo ""
          echo "Web deployment details:"
          kubectl describe deployment web -n cloudtolocalllm

          echo ""
          echo "Web pod logs (last 50 lines):"
          WEB_POD=$(kubectl get pods -n cloudtolocalllm -l app=web -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
          if [ -n "$WEB_POD" ]; then
            echo "Web pod: $WEB_POD"
            kubectl logs "$WEB_POD" -n cloudtolocalllm --tail=50 || echo "Could not retrieve logs"
          else
            echo "No web pods found"
          fi

          echo "::endgroup::"

  dns-validation:
    needs: deploy
    runs-on: ubuntu-latest
    environment: production
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Get Load Balancer IP
        id: lb-ip
        run: |
          az aks get-credentials \
            --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
            --name ${{ env.AZURE_CLUSTER_NAME }} \
            --overwrite-existing

          LB_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
            -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

          echo "Load Balancer IP: $LB_IP"
          echo "lb_ip=$LB_IP" >> $GITHUB_OUTPUT

      # - name: Configure Azure DNS (Optional - Backup)
      #   continue-on-error: true
      #   run: |
      #     echo "⚠️  Note: Using Cloudflare DNS for primary DNS management"
      #     echo "Azure DNS backup configuration disabled to streamline deployment"

      - name: DNS Health Check
        run: |
          echo "Performing DNS and API health check..."
          echo "Load Balancer IP: ${{ steps.lb-ip.outputs.lb_ip }}"

          # Wait up to 5 minutes for DNS to propagate and API to become healthy
          timeout=300
          interval=15

          # Try 3 times as requested
          for i in {1..3}; do
            echo "Attempting to reach API at https://api.cloudtolocalllm.online/health (Attempt $i/3)..."
            response_code=$(curl -s -o /dev/null -w "%{http_code}" https://api.cloudtolocalllm.online/health)
            
            if [ "$response_code" -eq 200 ]; then
              echo "✅ API is healthy and reachable! (Status: $response_code)"
              exit 0
            else
              echo "API not yet healthy (Status: $response_code). Retrying in $interval seconds..."
              sleep $interval
            fi
          done

          echo "❌ API health check failed after $timeout seconds."
          exit 1

      - name: Get API Backend Pod Logs on Failure
        if: failure()
        run: |
          echo "::group::API Backend Pod Logs (on health check failure)"
          echo "Attempting to retrieve logs for API backend pods..."
          API_POD_NAME=$(kubectl get pods -n cloudtolocalllm -l app=api-backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -n "$API_POD_NAME" ]; then
            echo "Found API pod: $API_POD_NAME"
            kubectl logs "$API_POD_NAME" -n cloudtolocalllm || echo "Could not retrieve logs for $API_POD_NAME"
          else
            echo "No API backend pods found to retrieve logs from."
          fi
          echo "::endgroup::"

      - name: Get Postgres Pod Logs on Failure
        if: failure()
        run: |
          echo "::group::Postgres Pod Logs (on health check failure)"
          echo "Attempting to retrieve logs for Postgres pods..."
          POSTGRES_POD_NAME=$(kubectl get pods -n cloudtolocalllm -l app=postgres -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -n "$POSTGRES_POD_NAME" ]; then
            echo "Found Postgres pod: $POSTGRES_POD_NAME"
            kubectl logs "$POSTGRES_POD_NAME" -n cloudtolocalllm || echo "Could not retrieve logs for $POSTGRES_POD_NAME"
            
            echo "--- Previous Logs (if restarted) ---"
            kubectl logs "$POSTGRES_POD_NAME" -n cloudtolocalllm --previous || echo "No previous logs"
          else
            echo "No Postgres pods found to retrieve logs from."
          fi
          echo "::endgroup::"
