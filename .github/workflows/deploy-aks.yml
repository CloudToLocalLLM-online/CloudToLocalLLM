name: Deploy to Azure AKS

permissions:
  id-token: write

on:
  push:
    branches:
      - main
    paths:
      - "k8s/**"
      - ".github/workflows/deploy-aks.yml"
      - "services/**"
      - "web/**"
      - "lib/**"
      - "pubspec.**"
      - "config/docker/**"
      - "scripts/**"
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AZURE_RESOURCE_GROUP: cloudtolocalllm-rg
  AZURE_CLUSTER_NAME: cloudtolocalllm-aks
  ACR_NAME: imrightguycloudtolocalllm
  # DOCKER_REGISTRY will be set dynamically to the ACR login server
  DOCKER_REGISTRY: ""

jobs:
  # Job to detect changes in specific paths
  changes:
    runs-on: ubuntu-latest
    outputs:
      base: ${{ steps.filter.outputs.base }}
      postgres: ${{ steps.filter.outputs.postgres }}
      web: ${{ steps.filter.outputs.web }}
      api: ${{ steps.filter.outputs.api }}
      proxy: ${{ steps.filter.outputs.proxy }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            base:
              - 'config/docker/Dockerfile.base'
              - '.github/workflows/deploy-aks.yml'
            postgres:
              - 'config/docker/Dockerfile.postgres'
              - 'config/docker/postgres-init.sh'
              - 'config/docker/postgres-entrypoint.sh'
              - '.github/workflows/deploy-aks.yml'
            web:
              - 'web/**'
              - 'lib/**'
              - 'pubspec.**'
              - 'config/docker/Dockerfile.base'
              - '.github/workflows/deploy-aks.yml'
            api:
              - 'services/api-backend/**'
              - 'config/docker/Dockerfile.base'
              - '.github/workflows/deploy-aks.yml'
            proxy:
              - 'services/streaming-proxy/**'
              - 'config/docker/Dockerfile.base'
              - '.github/workflows/deploy-aks.yml'

  build_base:
    needs: changes
    if: ${{ !cancelled() && needs.changes.outputs.base == 'true' }}
    runs-on: ubuntu-latest
    outputs:
      base_version: ${{ steps.set_version.outputs.base_version }}
    steps:
      - uses: actions/checkout@v4
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create Azure Resource Group if not exists
        run: az group create --name ${{ env.AZURE_RESOURCE_GROUP }} --location eastus --query "properties.provisioningState" --output tsv
      - name: Setup ACR
        run: |
          # Create ACR if not exists
          az acr create --resource-group ${{ env.AZURE_RESOURCE_GROUP }} --name ${{ env.ACR_NAME }} --sku Basic --admin-enabled true
          # Get Login Server
          echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV
      - name: Docker Login to ACR
        run: |
          # Get ACR admin credentials
          ACR_USERNAME=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_PASSWORD=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)
          # Login to ACR
          echo "$ACR_PASSWORD" | docker login ${{ env.DOCKER_REGISTRY }} --username "$ACR_USERNAME" --password-stdin
      - uses: docker/setup-buildx-action@v3
      
      - name: Set Base Version
        id: set_version
        run: |
          BASE_VERSION=$(jq -r '.version' assets/version.json)-base
          echo "base_version=$BASE_VERSION" >> $GITHUB_OUTPUT
          echo "✅ Base version: $BASE_VERSION"
      
      - uses: docker/build-push-action@v5
        with:
          context: .
          file: config/docker/Dockerfile.base
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/base:${{ steps.set_version.outputs.base_version }}
            ${{ env.DOCKER_REGISTRY }}/base:${{ github.sha }}
            ${{ env.DOCKER_REGISTRY }}/base:latest
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/base:latest
          cache-to: type=inline

  build_postgres:
    needs: [changes, build_base, build_web]
    if: ${{ !cancelled() && (needs.build_base.result == 'success' || needs.build_base.result == 'skipped') && needs.changes.outputs.postgres == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create Azure Resource Group if not exists
        run: az group create --name ${{ env.AZURE_RESOURCE_GROUP }} --location eastus --query "properties.provisioningState" --output tsv
      - name: Get ACR Login Server
        run: echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV
      - name: Docker Login to ACR
        run: |
          # Get ACR admin credentials
          ACR_USERNAME=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_PASSWORD=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)
          # Login to ACR
          echo "$ACR_PASSWORD" | docker login ${{ env.DOCKER_REGISTRY }} --username "$ACR_USERNAME" --password-stdin
      - uses: docker/setup-buildx-action@v3
      
      - name: Set Postgres Version
        run: |
          chmod +x scripts/get-next-service-version.sh
          
          # Get next version from ACR for postgres service
          NEXT_VERSION=$(./scripts/get-next-service-version.sh postgres ${{ env.ACR_NAME }})
          POSTGRES_VERSION="${NEXT_VERSION}-postgres"
          echo "POSTGRES_VERSION=$POSTGRES_VERSION" >> $GITHUB_ENV
          echo "✅ Postgres version: $POSTGRES_VERSION"
      
      - uses: docker/build-push-action@v5
        with:
          context: .
          file: config/docker/Dockerfile.postgres
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/postgres:${{ env.POSTGRES_VERSION }}
            ${{ env.DOCKER_REGISTRY }}/postgres:${{ github.sha }}
            ${{ env.DOCKER_REGISTRY }}/postgres:latest
          build-args: |
            BASE_REGISTRY=${{ env.DOCKER_REGISTRY }}
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/postgres:latest
          cache-to: type=inline

  build_web:
    needs: [changes, build_base]
    if: ${{ !cancelled() && (needs.build_base.result == 'success' || needs.build_base.result == 'skipped') && needs.changes.outputs.web == 'true' }}
    runs-on: ubuntu-latest
    outputs:
      app_version: ${{ steps.bump_version.outputs.app_version }}
      build_number: ${{ steps.bump_version.outputs.build_number }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Bump Web Version
        id: bump_version
        run: |
          chmod +x scripts/get-next-service-version.sh
          
          # Get next version from ACR for web service
          NEXT_VERSION=$(./scripts/get-next-service-version.sh web ${{ env.ACR_NAME }})
          BUILD_NUMBER=$(date +%Y%m%d%H%M)
          BUILD_DATE=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          GIT_COMMIT=$(git rev-parse --short HEAD)
          BUILD_TIMESTAMP=$(date +"%Y-%m-%d %H:%M:%S")
          
          # Update version.json using jq
          jq -n \
            --arg version "$NEXT_VERSION" \
            --arg build_number "$BUILD_NUMBER" \
            --arg build_date "$BUILD_DATE" \
            --arg git_commit "$GIT_COMMIT" \
            --arg buildTimestamp "$BUILD_TIMESTAMP" \
            '{
              version: $version,
              build_number: $build_number,
              build_date: $build_date,
              git_commit: $git_commit,
              buildTimestamp: $buildTimestamp
            }' > assets/version.json
          
          echo "app_version=$NEXT_VERSION" >> $GITHUB_OUTPUT
          echo "build_number=$BUILD_NUMBER" >> $GITHUB_OUTPUT
          
          echo "✅ Web version: $NEXT_VERSION"
          echo "✅ Build number: $BUILD_NUMBER"
      
      - name: Sync Component Versions
        run: |
          # Create component versions file for web to display
          WEB_VERSION=$(jq -r '.version' assets/version.json)
          
          # Query ACR for latest versions of all components
          API_VERSION=$(./scripts/get-next-service-version.sh api-backend ${{ env.ACR_NAME }} 2>/dev/null || echo "$WEB_VERSION")
          POSTGRES_VERSION=$(./scripts/get-next-service-version.sh postgres ${{ env.ACR_NAME }} 2>/dev/null || echo "$WEB_VERSION")
          PROXY_VERSION=$(./scripts/get-next-service-version.sh streaming-proxy ${{ env.ACR_NAME }} 2>/dev/null || echo "$WEB_VERSION")
          BASE_VERSION=$(./scripts/get-next-service-version.sh base ${{ env.ACR_NAME }} 2>/dev/null || echo "$WEB_VERSION")
          
          # Create component versions file using jq
          jq -n \
            --arg web "$WEB_VERSION" \
            --arg api "${API_VERSION}-api" \
            --arg postgres "${POSTGRES_VERSION}-postgres" \
            --arg streaming_proxy "${PROXY_VERSION}-proxy" \
            --arg base "${BASE_VERSION}-base" \
            --arg last_updated "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            '{
              web: $web,
              api: $api,
              postgres: $postgres,
              streaming_proxy: $streaming_proxy,
              base: $base,
              last_updated: $last_updated
            }' > assets/component-versions.json
          
          echo "✅ Component versions synced"
          cat assets/component-versions.json
      
      - name: Commit Version Bump
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add assets/version.json assets/component-versions.json
          git commit -m "chore: bump web to $(jq -r '.version' assets/version.json) and sync component versions [skip ci]" || echo "No version changes to commit"
          git push || echo "No changes to push"
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create Azure Resource Group if not exists
        run: az group create --name ${{ env.AZURE_RESOURCE_GROUP }} --location eastus --query "properties.provisioningState" --output tsv
      - name: Get ACR Login Server
        run: echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV
      - name: Docker Login to ACR
        run: |
          # Get ACR admin credentials
          ACR_USERNAME=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_PASSWORD=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)
          # Login to ACR
          echo "$ACR_PASSWORD" | docker login ${{ env.DOCKER_REGISTRY }} --username "$ACR_USERNAME" --password-stdin
      - uses: docker/setup-buildx-action@v3
      - name: Check if Flutter is already installed
        id: flutter_check
        run: |
          if command -v flutter >/dev/null 2>&1; then
            FLUTTER_VERSION=$(flutter --version | head -n1 || echo "unknown")
            echo "Flutter is already installed: $FLUTTER_VERSION"
            echo "flutter_installed=true" >> $GITHUB_OUTPUT
          else
            echo "Flutter not found, will install"
            echo "flutter_installed=false" >> $GITHUB_OUTPUT
          fi
      - name: Install Flutter (if needed)
        if: steps.flutter_check.outputs.flutter_installed != 'true'
        uses: subosito/flutter-action@v2
        with:
          channel: "stable"
          cache: true
      - name: Build Flutter Web
        run: |
          flutter pub get
          flutter build web --release --no-tree-shake-icons --base-href / --dart-define=FLUTTER_BUILD_NUMBER=${{ github.sha }}
      - name: Apply Cache Busting to index.html
        run: |
          # Replace placeholder with actual build number (Git SHA)
          sed -i "s|\$FLUTTER_BUILD_NUMBER|${{ github.sha }}|g" build/web/index.html
          # Ensure base href is correctly set for root path
          sed -i "s|\$FLUTTER_BASE_HREF|/|g" build/web/index.html
      - name: Debug - Verify index.html cache busting
        run: |
          echo "Checking if cache-busting was applied:"
          grep "flutter_bootstrap.js" build/web/index.html
      - name: Debug - List web build
        run: ls -R build/web || echo "build/web not found"
      - uses: docker/build-push-action@v5
        with:
          context: .
          file: web/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/web:${{ steps.bump_version.outputs.app_version }}
            ${{ env.DOCKER_REGISTRY }}/web:${{ github.sha }}
            ${{ env.DOCKER_REGISTRY }}/web:latest
          build-args: |
            BASE_REGISTRY=${{ env.DOCKER_REGISTRY }}
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/web:latest
          cache-to: type=inline
  
  build_api:
    needs: [changes, build_base, build_web]
    if: ${{ !cancelled() && (needs.build_base.result == 'success' || needs.build_base.result == 'skipped') && needs.changes.outputs.api == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create Azure Resource Group if not exists
        run: az group create --name ${{ env.AZURE_RESOURCE_GROUP }} --location eastus --query "properties.provisioningState" --output tsv
      - name: Get ACR Login Server
        run: echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV
      - name: Docker Login to ACR
        run: |
          # Get ACR admin credentials
          ACR_USERNAME=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_PASSWORD=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)
          # Login to ACR
          echo "$ACR_PASSWORD" | docker login ${{ env.DOCKER_REGISTRY }} --username "$ACR_USERNAME" --password-stdin
      - uses: docker/setup-buildx-action@v3
      
      - name: Set API Version
        run: |
          chmod +x scripts/get-next-service-version.sh
          
          # Get next version from ACR for api-backend service
          NEXT_VERSION=$(./scripts/get-next-service-version.sh api-backend ${{ env.ACR_NAME }})
          API_VERSION="${NEXT_VERSION}-api"
          echo "API_VERSION=$API_VERSION" >> $GITHUB_ENV
          echo "✅ API version: $API_VERSION"
      
      - uses: docker/build-push-action@v5
        with:
          context: services/api-backend
          file: services/api-backend/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/api-backend:${{ env.API_VERSION }}
            ${{ env.DOCKER_REGISTRY }}/api-backend:${{ github.sha }}
            ${{ env.DOCKER_REGISTRY }}/api-backend:latest
          build-args: |
            BASE_REGISTRY=${{ env.DOCKER_REGISTRY }}
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/api-backend:latest
          cache-to: type=inline

  build_proxy:
    needs: [changes, build_base, build_web]
    if: ${{ !cancelled() && (needs.build_base.result == 'success' || needs.build_base.result == 'skipped') && needs.changes.outputs.proxy == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create Azure Resource Group if not exists
        run: az group create --name ${{ env.AZURE_RESOURCE_GROUP }} --location eastus --query "properties.provisioningState" --output tsv
      - name: Get ACR Login Server
        run: echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV
      - name: Docker Login to ACR
        run: |
          # Get ACR admin credentials
          ACR_USERNAME=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_PASSWORD=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)
          # Login to ACR
          echo "$ACR_PASSWORD" | docker login ${{ env.DOCKER_REGISTRY }} --username "$ACR_USERNAME" --password-stdin
      - uses: docker/setup-buildx-action@v3
      
      - name: Set Proxy Version
        run: |
          chmod +x scripts/get-next-service-version.sh
          
          # Get next version from ACR for streaming-proxy service
          NEXT_VERSION=$(./scripts/get-next-service-version.sh streaming-proxy ${{ env.ACR_NAME }})
          PROXY_VERSION="${NEXT_VERSION}-proxy"
          echo "PROXY_VERSION=$PROXY_VERSION" >> $GITHUB_ENV
          echo "✅ Proxy version: $PROXY_VERSION"
      
      - uses: docker/build-push-action@v5
        with:
          context: services/streaming-proxy
          file: services/streaming-proxy/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/streaming-proxy:${{ env.PROXY_VERSION }}
            ${{ env.DOCKER_REGISTRY }}/streaming-proxy:${{ github.sha }}
            ${{ env.DOCKER_REGISTRY }}/streaming-proxy:latest
          build-args: |
            BASE_REGISTRY=${{ env.DOCKER_REGISTRY }}
          cache-from: type=registry,ref=${{ env.DOCKER_REGISTRY }}/streaming-proxy:latest
          cache-to: type=inline

  deploy_infrastructure:
    permissions:
      id-token: write
    # Run after image builds (or if they were skipped due to unchanged paths)
    needs:
      [changes, build_base, build_postgres, build_web, build_api, build_proxy]
    if: |
      !cancelled() &&
      (needs.build_base.result == 'success' || needs.build_base.result == 'skipped') &&
      (needs.build_postgres.result == 'success' || needs.build_postgres.result == 'skipped') &&
      (needs.build_web.result == 'success' || needs.build_web.result == 'skipped') &&
      (needs.build_api.result == 'success' || needs.build_api.result == 'skipped') &&
      (needs.build_proxy.result == 'success' || needs.build_proxy.result == 'skipped')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Bootstrap Azure RG/ACR/Key Vault
        run: |
          chmod +x scripts/bootstrap-azure-infra.sh
          scripts/bootstrap-azure-infra.sh "${{ env.AZURE_RESOURCE_GROUP }}" "eastus" "${{ env.ACR_NAME }}" "${{ secrets.AZURE_KEY_VAULT_NAME }}"
      - name: Create AKS Cluster if not exists
        run: |
          if az aks show --resource-group ${{ env.AZURE_RESOURCE_GROUP }} --name ${{ env.AZURE_CLUSTER_NAME }} --query "name" --output tsv >/dev/null 2>&1; then
            echo "✅ AKS cluster ${{ env.AZURE_CLUSTER_NAME }} already exists"
            CLUSTER_STATE=$(az aks show --resource-group ${{ env.AZURE_RESOURCE_GROUP }} --name ${{ env.AZURE_CLUSTER_NAME }} --query "provisioningState" --output tsv)
            echo "Cluster state: $CLUSTER_STATE"
            if [ "$CLUSTER_STATE" != "Succeeded" ]; then
              echo "⚠️  Warning: Cluster is not in Succeeded state"
            fi
          else
            echo "AKS cluster ${{ env.AZURE_CLUSTER_NAME }} not found. Creating..."
            az aks create \
              --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
              --name ${{ env.AZURE_CLUSTER_NAME }} \
              --node-count 1 \
              --enable-addons monitoring \
              --enable-managed-identity \
              --enable-oidc-issuer \
              --enable-workload-identity \
              --network-plugin azure \
              --dns-service-ip 10.2.0.10 \
              --service-cidr 10.2.0.0/24 \
              --location eastus \
              --generate-ssh-keys \
              --attach-acr ${{ env.ACR_NAME }}
            echo "✅ AKS cluster created successfully"
          fi
      
      - name: Get AKS credentials
        uses: azure/aks-set-context@v3
        with:
          resource-group: ${{ env.AZURE_RESOURCE_GROUP }}
          cluster-name: ${{ env.AZURE_CLUSTER_NAME }}

      - name: Validate Required CLI Tools
        run: |
          set -e

          echo "Validating required CLI tools (az, kubectl, jq)..."

          missing=0
          for cmd in az kubectl jq; do
            if ! command -v "$cmd" >/dev/null 2>&1; then
              echo "Error: required CLI tool '$cmd' is not installed or not in PATH"
              missing=1
            else
              echo "Found $cmd: $(command -v "$cmd")"
            fi
          done

          if [ "$missing" -ne 0 ]; then
            echo "One or more required CLI tools are missing. Failing before deployment."
            exit 1
          fi

      - name: Setup ACR Integration
        run: |
          # Get Login Server (ACR assumed to exist or created in build steps)
          echo "DOCKER_REGISTRY=$(az acr show --name ${{ env.ACR_NAME }} --query loginServer --output tsv)" >> $GITHUB_ENV

      - name: Configure Key Vault RBAC for AKS
        continue-on-error: true
        run: |
          set -e
          KV_NAME="${{ secrets.AZURE_KEY_VAULT_NAME }}"
          if [ -z "$KV_NAME" ]; then
            echo "Key Vault name not set, skipping RBAC configuration"
            exit 0
          fi
          KV_ID=$(az keyvault show -g ${{ env.AZURE_RESOURCE_GROUP }} -n "$KV_NAME" --query id -o tsv 2>/dev/null || echo "")
          if [ -z "$KV_ID" ]; then
            echo "Key Vault not found, skipping RBAC configuration"
            exit 0
          fi
          AKS_MI_ID=$(az aks show -g ${{ env.AZURE_RESOURCE_GROUP }} -n ${{ env.AZURE_CLUSTER_NAME }} --query identityProfile.kubeletidentity.objectId -o tsv 2>/dev/null || echo "")
          if [ -z "$AKS_MI_ID" ]; then
            AKS_MI_ID=$(az aks show -g ${{ env.AZURE_RESOURCE_GROUP }} -n ${{ env.AZURE_CLUSTER_NAME }} --query identity.principalId -o tsv 2>/dev/null || echo "")
          fi
          if [ -z "$AKS_MI_ID" ]; then
            echo "AKS managed identity not found, skipping RBAC configuration"
            exit 0
          fi
          echo "AKS Managed Identity Object ID: $AKS_MI_ID"
          
          ROLE="Key Vault Secrets User"
          COUNT=$(az role assignment list --assignee-object-id "$AKS_MI_ID" --scope "$KV_ID" --query "[?roleDefinitionName=='$ROLE'] | length(@)" -o tsv 2>/dev/null || echo "0")
          
          if [ "$COUNT" = "0" ] || [ -z "$COUNT" ]; then
            echo "Attempting to assign Key Vault RBAC role..."
            # Use --assignee-object-id and --assignee-principal-type to avoid Graph API lookup
            if az role assignment create \
              --assignee-object-id "$AKS_MI_ID" \
              --assignee-principal-type "ServicePrincipal" \
              --role "$ROLE" \
              --scope "$KV_ID" 2>&1; then
              echo "✅ Successfully assigned Key Vault RBAC role"
            else
              echo "⚠️  WARNING: Could not assign Key Vault RBAC role (insufficient permissions)"
              echo "    This may be okay if using Kubernetes secrets instead of Key Vault CSI"
              echo "    To fix manually, grant 'Key Vault Secrets User' role to: $AKS_MI_ID"
              exit 0
            fi
          else
            echo "✅ Key Vault RBAC role already assigned"
          fi

      - name: Ensure Namespace Exists
        run: |
          kubectl create namespace cloudtolocalllm --dry-run=client -o yaml | kubectl apply -f -
      - name: Setup Azure Key Vault and Secrets Store CSI Driver
        run: |
          set -e
          # Install the Secrets Store CSI Driver and Azure Provider
          helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts 2>/dev/null || true
          helm repo add csi-secrets-store-provider-azure https://azure.github.io/secrets-store-csi-driver-provider-azure/charts 2>/dev/null || true
          helm repo update
          
          # Check if CSI driver is already installed and healthy
          if helm status csi-secrets-store -n kube-system >/dev/null 2>&1; then
            HELM_STATUS=$(helm status csi-secrets-store -n kube-system -o json | jq -r '.info.status' 2>/dev/null || echo "unknown")
            if [ "$HELM_STATUS" = "deployed" ]; then
              echo "✅ CSI Secret Store driver is already deployed and healthy"
              echo "Skipping installation"
              exit 0
            else
              echo "⚠️  CSI Secret Store driver exists but status is: $HELM_STATUS"
              echo "Will attempt to fix..."
            fi
          else
            echo "CSI Secret Store driver not found, will install"
          fi
          
          # Function to check and clear Helm locks
          clear_helm_locks() {
            local release_name="$1"
            local namespace="$2"
            
            echo "Checking for Helm locks for release: $release_name in namespace: $namespace"
            
            # Check if release exists and is in pending state
            local release_status=$(helm status "$release_name" -n "$namespace" 2>&1 || echo "not_found")
            if echo "$release_status" | grep -q "pending"; then
              echo "⚠️  Found pending Helm release, attempting to clear..."
              # Try to rollback to clear the pending state
              helm rollback "$release_name" -n "$namespace" 2>/dev/null || true
              sleep 5
            fi
            
            # Check for stuck releases and delete them if needed
            local stuck_release=$(helm list -n "$namespace" --pending 2>/dev/null | grep "$release_name" || echo "")
            if [ -n "$stuck_release" ]; then
              echo "⚠️  Found stuck Helm release, attempting to uninstall..."
              helm uninstall "$release_name" -n "$namespace" --ignore-not-found || true
              sleep 5
            fi
          }
          
          # Clear any existing locks for csi-secrets-store
          clear_helm_locks "csi-secrets-store" "kube-system"
          
          # Wait for any concurrent operations to complete
          echo "Waiting for any concurrent Helm operations to complete..."
          MAX_WAIT=120
          WAITED=0
          while [ $WAITED -lt $MAX_WAIT ]; do
            if ! helm list -n kube-system --pending 2>/dev/null | grep -q "csi-secrets-store"; then
              echo "✅ No pending operations detected"
              break
            fi
            echo "Still waiting for pending operations... (${WAITED}s/${MAX_WAIT}s)"
            sleep 5
            WAITED=$((WAITED + 5))
          done
          
          if [ $WAITED -ge $MAX_WAIT ]; then
            echo "⚠️  Timeout waiting for pending operations, attempting to force clear..."
            helm uninstall csi-secrets-store -n kube-system --ignore-not-found || true
            sleep 10
          fi
          
          # Install/upgrade with retry logic
          RETRIES=5
          for i in $(seq 1 $RETRIES); do
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Attempt $i/$RETRIES: Installing CSI Secret Store driver..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            # Clear locks before each attempt
            if [ $i -gt 1 ]; then
              clear_helm_locks "csi-secrets-store" "kube-system"
              sleep 10
            fi
            
            if helm upgrade --install csi-secrets-store \
              secrets-store-csi-driver/secrets-store-csi-driver \
              --namespace kube-system \
              --set "providers.azure.enabled=true" \
              --wait \
              --timeout 10m \
              --atomic; then
              echo "✅ CSI Secret Store driver installed successfully"
              break
            else
              local exit_code=$?
              echo "⚠️  Attempt $i failed with exit code: $exit_code"
              
              if [ $i -lt $RETRIES ]; then
                echo "Waiting 15s before retry..."
                sleep 15
              else
                echo "❌ Failed after $RETRIES attempts"
                echo "Checking release status..."
                helm status csi-secrets-store -n kube-system || true
                helm list -n kube-system || true
                exit 1
              fi
            fi
          done

      - name: Create SecretProviderClass
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: secrets-store.csi.x-k8s.io/v1
          kind: SecretProviderClass
          metadata:
            name: azure-kvname-user-msi
            namespace: cloudtolocalllm
          spec:
            provider: azure
            parameters:
              usePodIdentity: "false"
              useVMManagedIdentity: "true"
              userAssignedIdentityID: "" # Set this to the client ID of the user-assigned managed identity if you use one.
              keyvaultName: ${{ secrets.AZURE_KEY_VAULT_NAME }}
              objects:  |
                array:
                  - |
                    objectName: POSTGRES-PASSWORD
                    objectType: secret
                  - |
                    objectName: JWT-SECRET
                    objectType: secret
                  - |
                    objectName: STRIPE-TEST-SECRET-KEY
                    objectType: secret
                  - |
                    objectName: CLOUDFLARE-TUNNEL-TOKEN
                    objectType: secret
              tenantId: ${{ secrets.AZURE_TENANT_ID }}
          EOF

      - name: Validate Required Secrets
        env:
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          JWT_SECRET: ${{ secrets.JWT_SECRET }}
          STRIPE_TEST_SECRET_KEY: ${{ secrets.STRIPE_TEST_SECRET_KEY }}
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
          SUPABASE_JWT_SECRET: ${{ secrets.SUPABASE_JWT_SECRET }}
        run: |
          missing=0
          check_secret() {
            name="$1"
            value="$2"
            if [ -z "$value" ]; then
              echo "Error: required secret $name is empty or not set"
              missing=1
            fi
          }

          check_secret POSTGRES_PASSWORD "$POSTGRES_PASSWORD"
          check_secret JWT_SECRET "$JWT_SECRET"
          check_secret STRIPE_TEST_SECRET_KEY "$STRIPE_TEST_SECRET_KEY"
          check_secret SENTRY_DSN "$SENTRY_DSN"
          check_secret CLOUDFLARE_API_TOKEN "$CLOUDFLARE_API_TOKEN"
          check_secret CLOUDFLARE_TUNNEL_TOKEN "$CLOUDFLARE_TUNNEL_TOKEN"
          check_secret SUPABASE_JWT_SECRET "$SUPABASE_JWT_SECRET"

          if [ "$missing" -ne 0 ]; then
            echo "One or more required secrets are missing. Failing before deployment."
            exit 1
          fi

      - name: Seed Azure Key Vault from secrets
        env:
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          JWT_SECRET: ${{ secrets.JWT_SECRET }}
          STRIPE_TEST_SECRET_KEY: ${{ secrets.STRIPE_TEST_SECRET_KEY }}
          CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
          SUPABASE_JWT_SECRET: ${{ secrets.SUPABASE_JWT_SECRET }}
        run: |
          set -e
          KV_NAME="${{ secrets.AZURE_KEY_VAULT_NAME }}"
          if [ -z "$KV_NAME" ]; then
            exit 0
          fi
          ensure_kv_secret() {
            local name="$1"
            local value="$2"
            if [ -z "$value" ]; then
              return
            fi
            if az keyvault secret show --vault-name "$KV_NAME" --name "$name" >/dev/null 2>&1; then
              return
            fi
            az keyvault secret set --vault-name "$KV_NAME" --name "$name" --value "$value" >/dev/null
          }
          ensure_kv_secret POSTGRES-PASSWORD "$POSTGRES_PASSWORD"
          ensure_kv_secret JWT-SECRET "$JWT_SECRET"
          ensure_kv_secret STRIPE-TEST-SECRET-KEY "$STRIPE_TEST_SECRET_KEY"
          ensure_kv_secret CLOUDFLARE-TUNNEL-TOKEN "$CLOUDFLARE_TUNNEL_TOKEN"
          ensure_kv_secret SUPABASE-JWT-SECRET "$SUPABASE_JWT_SECRET"

      - name: Create/Update Secrets
        env:
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          JWT_SECRET: ${{ secrets.JWT_SECRET }}
          STRIPE_TEST_SECRET_KEY: ${{ secrets.STRIPE_TEST_SECRET_KEY }}
          STRIPE_TEST_PUBLISHABLE_KEY: ${{ secrets.STRIPE_TEST_PUBLISHABLE_KEY }}
          STRIPE_TEST_WEBHOOK_SECRET: ${{ secrets.STRIPE_TEST_WEBHOOK_SECRET }}
          STRIPE_LIVE_SECRET_KEY: ${{ secrets.STRIPE_LIVE_SECRET_KEY }}
          STRIPE_LIVE_PUBLISHABLE_KEY: ${{ secrets.STRIPE_LIVE_PUBLISHABLE_KEY }}
          STRIPE_LIVE_WEBHOOK_SECRET: ${{ secrets.STRIPE_LIVE_WEBHOOK_SECRET }}
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          CLOUDFLARE_DNS_TOKEN: ${{ secrets.CLOUDFLARE_DNS_TOKEN }}
          CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          SUPABASE_JWT_SECRET: ${{ secrets.SUPABASE_JWT_SECRET }}
        run: |
          # --- Create Kubernetes Secrets ---

          # Get ACR Admin Credentials
          ACR_ADMIN_USER=$(az acr credential show --name ${{ env.ACR_NAME }} --query username --output tsv)
          ACR_ADMIN_PASS=$(az acr credential show --name ${{ env.ACR_NAME }} --query passwords[0].value --output tsv)

          # Docker Registry Secret for ACR
          kubectl create secret docker-registry regcred \
            --docker-server=${{ env.DOCKER_REGISTRY }} \
            --docker-username=$ACR_ADMIN_USER \
            --docker-password=$ACR_ADMIN_PASS \
            --docker-email=github-actions@cloudtolocalllm.online \
            -n cloudtolocalllm --dry-run=client -o yaml | kubectl apply -f -

          # App Secrets
          kubectl create secret generic cloudtolocalllm-secrets \
            --namespace cloudtolocalllm \
            --from-literal=postgres-user=cloud_admin \
            --from-literal=postgres-password="$POSTGRES_PASSWORD" \
            --from-literal=jwt-secret="$JWT_SECRET" \
            --from-literal=stripe-test-secret-key="$STRIPE_TEST_SECRET_KEY" \
            --from-literal=stripe-test-publishable-key="$STRIPE_TEST_PUBLISHABLE_KEY" \
            --from-literal=stripe-test-webhook-secret="$STRIPE_TEST_WEBHOOK_SECRET" \
            --from-literal=stripe-live-secret-key="$STRIPE_LIVE_SECRET_KEY" \
            --from-literal=stripe-live-publishable-key="$STRIPE_LIVE_PUBLISHABLE_KEY" \
            --from-literal=stripe-live-webhook-secret="$STRIPE_LIVE_WEBHOOK_SECRET" \
            --from-literal=sentry-dsn="$SENTRY_DSN" \
            --from-literal=supabase-jwt-secret="$SUPABASE_JWT_SECRET" \
            --from-literal=supabase-url="https://cvaqhvpbnemweqrojasm.supabase.co" \
            --dry-run=client -o yaml | kubectl apply -f -

          # Cloudflare Tunnel Secret
          kubectl create secret generic tunnel-credentials \
            --namespace cloudtolocalllm \
            --from-literal=token="$CLOUDFLARE_TUNNEL_TOKEN" \
            --dry-run=client -o yaml | kubectl apply -f -

          # Streaming Proxy Secret
          kubectl create secret generic streaming-proxy-secrets \
            --namespace cloudtolocalllm \
            --from-literal=SENTRY_DSN="$SENTRY_DSN" \
            --from-literal=SUPABASE_JWT_SECRET="$SUPABASE_JWT_SECRET" \
            --from-literal=SUPABASE_URL="https://cvaqhvpbnemweqrojasm.supabase.co" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy Manifests
        env:
          POSTGRES_BUILT: ${{ needs.changes.outputs.postgres }}
          WEB_BUILT: ${{ needs.changes.outputs.web }}
          API_BUILT: ${{ needs.changes.outputs.api }}
          PROXY_BUILT: ${{ needs.changes.outputs.proxy }}
          POSTGRES_VERSION: ${{ needs.build_postgres.outputs.postgres_version }}
          WEB_VERSION: ${{ needs.build_web.outputs.app_version }}
          API_VERSION: ${{ needs.build_api.outputs.api_version }}
          PROXY_VERSION: ${{ needs.build_proxy.outputs.proxy_version }}
        run: |
          set -e  # Exit immediately on any error
          
          # Use semantic version tag if service was built, otherwise use latest
          # Web build determines the app version - it's the source of truth
          POSTGRES_TAG="${POSTGRES_VERSION:-latest}"
          WEB_TAG="${WEB_VERSION:-latest}"
          API_TAG="${API_VERSION:-latest}"
          PROXY_TAG="${PROXY_VERSION:-latest}"
          
          # Override with latest if service wasn't built this run
          [[ "$POSTGRES_BUILT" != "true" ]] && POSTGRES_TAG="latest"
          [[ "$WEB_BUILT" != "true" ]] && WEB_TAG="latest"
          [[ "$API_BUILT" != "true" ]] && API_TAG="latest"
          [[ "$PROXY_BUILT" != "true" ]] && PROXY_TAG="latest"
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Image Versions for Deployment"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  Postgres: $POSTGRES_TAG"
          echo "  Web:      $WEB_TAG (Source of Truth)"
          echo "  API:      $API_TAG"
          echo "  Proxy:    $PROXY_TAG"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""
          
          # Get current app version (web is source of truth)
          APP_VERSION=$(jq -r '.version' assets/version.json 2>/dev/null || echo "unknown")
          BUILD_NUMBER=$(jq -r '.build_number' assets/version.json 2>/dev/null || echo "unknown")
          GIT_COMMIT=$(jq -r '.git_commit' assets/version.json 2>/dev/null || echo "$(git rev-parse --short HEAD)")
          
          echo "App Version (from web): $APP_VERSION"
          
          # Replace image registry in manifests with appropriate tags
          sed -i "s|cloudtolocalllm/postgres:latest|${{ env.DOCKER_REGISTRY }}/postgres:${POSTGRES_TAG}|g" k8s/postgres-statefulset.yaml
          sed -i "s|cloudtolocalllm/web:latest|${{ env.DOCKER_REGISTRY }}/web:${WEB_TAG}|g" k8s/web-deployment.yaml
          sed -i "s|cloudtolocalllm/api-backend:latest|${{ env.DOCKER_REGISTRY }}/api-backend:${API_TAG}|g" k8s/api-backend-deployment.yaml
          sed -i "s|cloudtolocalllm/streaming-proxy:latest|${{ env.DOCKER_REGISTRY }}/streaming-proxy:${PROXY_TAG}|g" k8s/streaming-proxy-deployment.yaml
          
          # Inject version environment variables into API and Proxy deployments
          # Add API_VERSION env var to api-backend
          yq e '.spec.template.spec.containers[0].env += [{"name": "API_VERSION", "value": "'$APP_VERSION'-api"}, {"name": "BUILD_NUMBER", "value": "'$BUILD_NUMBER'"}, {"name": "GIT_COMMIT", "value": "'$GIT_COMMIT'"}]' -i k8s/api-backend-deployment.yaml
          
          # Add PROXY_VERSION env var to streaming-proxy
          yq e '.spec.template.spec.containers[0].env += [{"name": "PROXY_VERSION", "value": "'$APP_VERSION'-proxy"}, {"name": "BUILD_NUMBER", "value": "'$BUILD_NUMBER'"}, {"name": "GIT_COMMIT", "value": "'$GIT_COMMIT'"}]' -i k8s/streaming-proxy-deployment.yaml

          # Validate manifests with server-side dry-run before applying
          echo "Validating Kubernetes manifests with server-side dry-run..."
          kubectl apply --dry-run=server -f k8s/rbac.yaml
          kubectl apply --dry-run=server -f k8s/configmap.yaml
          kubectl apply --dry-run=server -f k8s/streaming-proxy-configmap.yaml
          kubectl apply --dry-run=server -f k8s/postgres-statefulset.yaml
          kubectl apply --dry-run=server -f k8s/cloudflared.yaml
          kubectl apply --dry-run=server -f k8s/web-deployment.yaml
          kubectl apply --dry-run=server -f k8s/api-backend-deployment.yaml
          kubectl apply --dry-run=server -f k8s/streaming-proxy-deployment.yaml

          # Apply RBAC and ConfigMaps first
          kubectl apply -f k8s/rbac.yaml
          kubectl apply -f k8s/configmap.yaml
          kubectl apply -f k8s/streaming-proxy-configmap.yaml

          # ===== STEP 1: Deploy and wait for PostgreSQL (all services depend on it) =====
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "STEP 1: Deploying PostgreSQL (all services depend on this)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          kubectl apply -f k8s/postgres-statefulset.yaml
          
          # Add annotation to force update
          kubectl annotate statefulset postgres -n cloudtolocalllm \
            deployment.kubernetes.io/revision="${{ github.sha }}" \
            deployment.kubernetes.io/timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --overwrite
          
          # Force delete postgres pod to ensure it picks up new spec (if it exists)
          kubectl delete pod postgres-0 -n cloudtolocalllm --force --grace-period=0 2>/dev/null || true
          kubectl rollout restart statefulset/postgres -n cloudtolocalllm
          
          # Wait for postgres to be ready before proceeding
          echo "Waiting for PostgreSQL to be ready..."
          chmod +x scripts/watch_rollout.sh
          ./scripts/watch_rollout.sh cloudtolocalllm statefulset postgres
          
          echo "✅ PostgreSQL is ready"
          echo ""
          
          # ===== STEP 2: Deploy API Backend (depends on postgres) =====
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "STEP 2: Deploying API Backend (depends on postgres)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          kubectl apply -f k8s/api-backend-deployment.yaml
          
          # Add annotation to force update
          kubectl annotate deployment api-backend -n cloudtolocalllm \
            deployment.kubernetes.io/revision="${{ github.sha }}" \
            deployment.kubernetes.io/timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --overwrite
          
          # Trigger rolling update
          kubectl rollout restart deployment/api-backend -n cloudtolocalllm
          
          echo "Waiting for API Backend to be ready..."
          ./scripts/watch_rollout.sh cloudtolocalllm deployment api-backend
          echo "✅ API Backend is ready"
          echo ""
          
          # ===== STEP 3: Deploy Streaming Proxy (depends on api-backend) =====
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "STEP 3: Deploying Streaming Proxy (depends on api-backend)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          kubectl apply -f k8s/streaming-proxy-deployment.yaml
          
          # Add annotation to force update
          kubectl annotate deployment streaming-proxy -n cloudtolocalllm \
            deployment.kubernetes.io/revision="${{ github.sha }}" \
            deployment.kubernetes.io/timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --overwrite
          
          # Trigger rolling update
          kubectl rollout restart deployment/streaming-proxy -n cloudtolocalllm
          
          echo "Waiting for Streaming Proxy to be ready..."
          ./scripts/watch_rollout.sh cloudtolocalllm deployment streaming-proxy
          echo "✅ Streaming Proxy is ready"
          echo ""
          
          # ===== STEP 4: Deploy Web and Cloudflared (depend on api-backend and streaming) =====
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "STEP 4: Deploying Web Frontend and Cloudflared"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          kubectl apply -f k8s/web-deployment.yaml
          kubectl apply -f k8s/cloudflared.yaml
          
          # Add annotations to force updates
          kubectl annotate deployment web -n cloudtolocalllm \
            deployment.kubernetes.io/revision="${{ github.sha }}" \
            deployment.kubernetes.io/timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --overwrite
          
          kubectl annotate deployment cloudflared -n cloudtolocalllm \
            deployment.kubernetes.io/revision="${{ github.sha }}" \
            deployment.kubernetes.io/timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --overwrite
          
          # Trigger rolling updates
          kubectl rollout restart deployment/web -n cloudtolocalllm
          kubectl rollout restart deployment/cloudflared -n cloudtolocalllm
          
          echo "Waiting for Web to be ready..."
          ./scripts/watch_rollout.sh cloudtolocalllm deployment web
          echo "✅ Web is ready"
          echo ""
          
          echo "✅ All services deployed successfully in dependency order:"
          echo "   1. PostgreSQL"
          echo "   2. API Backend"
          echo "   3. Streaming Proxy"
          echo "   4. Web + Cloudflared"

      - name: Purge Cloudflare Cache
        continue-on-error: true
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          set -e
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Purging Cloudflare Cache"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Get Zone ID for cloudtolocalllm.online (using scoped API token)
          DOMAIN="cloudtolocalllm.online"
          echo "Fetching Zone ID for: $DOMAIN"
          
          ZONE_RESPONSE=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones?name=$DOMAIN" \
            -H "Authorization: Bearer ${CLOUDFLARE_API_TOKEN}" \
            -H "Content-Type: application/json")
          
          ZONE_ID=$(echo "$ZONE_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | cut -d'"' -f4)
          
          if [ -z "$ZONE_ID" ]; then
            echo "❌ Failed to get Zone ID"
            echo "Response: $ZONE_RESPONSE"
            exit 1
          fi
          
          echo "✅ Zone ID: $ZONE_ID"
          echo "✅ Using scoped API token (Zone Read + Cache Purge permissions only)"
          echo ""
          
          # Purge everything for the zone (covers all subdomains)
          echo "Purging cache for entire zone (all domains)..."
          
          RESPONSE=$(curl -s -X POST \
            "https://api.cloudflare.com/client/v4/zones/${ZONE_ID}/purge_cache" \
            -H "Authorization: Bearer ${CLOUDFLARE_API_TOKEN}" \
            -H "Content-Type: application/json" \
            --data '{"purge_everything": true}')
          
          # Check if successful (note the space after colon in Cloudflare's response)
          if echo "$RESPONSE" | grep -q '"success": true'; then
            echo "✅ Cache purged successfully for all domains:"
            echo "   - cloudtolocalllm.online"
            echo "   - app.cloudtolocalllm.online"
            echo "   - api.cloudtolocalllm.online"
            echo ""
            echo "✅ Users will now receive the latest deployed version"
          else
            echo "⚠️  Cache purge failed - deployment will continue"
            echo "Response: $RESPONSE"
            echo ""
            echo "To fix this, update your Cloudflare API token permissions:"
            echo "1. Go to https://dash.cloudflare.com/profile/api-tokens"
            echo "2. Edit your API token"
            echo "3. Add permission: Zone > Cache Purge > Purge"
            echo "4. Update GitHub secret: gh secret set CLOUDFLARE_API_TOKEN"
            echo ""
            echo "For now, users may see cached content until it expires or they hard refresh (Ctrl+Shift+R)"
          fi

      - name: Verify Cloudflare Tunnel
        run: |
          set -e

          echo "Waiting for Cloudflare Tunnel pod to be ready..."
          kubectl wait --for=condition=ready pod -l app=cloudflared -n cloudtolocalllm --timeout=300s

          echo "Checking Tunnel Logs for successful registration..."
          start_ts=$(date +%s)
          # Retry checking logs for connection success message (24 attempts = ~2 minutes)
          for i in {1..24}; do
            if kubectl logs -l app=cloudflared -n cloudtolocalllm | grep -q "Registered tunnel connection"; then
              elapsed=$(( $(date +%s) - start_ts ))
              echo "Tunnel connection verified after ${elapsed}s."
              exit 0
            fi
            echo "Attempt $i/24: Tunnel not yet registered, waiting 5s..."
            sleep 5
          done

          echo "Tunnel verification failed after 24 attempts (~2 minutes). Collecting diagnostics..."
          echo "--- cloudflared pods ---"
          kubectl get pods -l app=cloudflared -n cloudtolocalllm -o wide || true
          echo "--- recent cloudflared logs ---"
          kubectl logs -l app=cloudflared -n cloudtolocalllm --tail=200 || true
          echo "--- describe cloudflared pods ---"
          kubectl describe pod -l app=cloudflared -n cloudtolocalllm || true
          exit 1

      - name: Update Cloudflare DNS
        if: false  # DNS is already configured - only run if manual DNS changes are needed
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
        run: |
          set -x
          # --- Update Cloudflare DNS Records ---
          # NOTE: DNS records are typically set up once and don't need updates on every deployment
          # This step is disabled by default. Enable it if DNS changes are needed.
          
          ZONE_NAME="cloudtolocalllm.online"
          TUNNEL_ID="62da6c19-947b-4bf6-acad-100a73de4e0d"
          TUNNEL_CNAME="$TUNNEL_ID.cfargotunnel.com"
          TUNNEL_TOKEN="${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}"

          if [ -z "$TUNNEL_TOKEN" ]; then
            echo "Error: CLOUDFLARE_TUNNEL_TOKEN secret is empty"
            exit 1
          fi

          echo "Fetching Zone ID for $ZONE_NAME..."
          ZONE_ID=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones?name=$ZONE_NAME" \
            -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
            -H "Content-Type: application/json" | jq -r '.result[0].id')
          
          if [ "$ZONE_ID" == "null" ] || [ -z "$ZONE_ID" ]; then
            echo "Error: Could not find Zone ID for $ZONE_NAME"
            exit 1
          fi

          # Function to update or create CNAME record
          update_dns_record() {
            local NAME=$1
            local CONTENT=$2
            local PROXIED=$3
            local FQDN

            if [ "$NAME" = "@" ]; then
              FQDN="$ZONE_NAME"
            else
              FQDN="$NAME.$ZONE_NAME"
            fi

            local RECORD_JSON=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records?type=CNAME&name=$FQDN" \
              -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
              -H "Content-Type: application/json")

            local RECORD_ID=$(echo "$RECORD_JSON" | jq -r '.result[0].id')
            local CURRENT_CONTENT=$(echo "$RECORD_JSON" | jq -r '.result[0].content')

            if [ "$CURRENT_CONTENT" = "$CONTENT" ]; then
              echo "DNS record $FQDN already points to $CONTENT; no update required."
              return 0
            fi

            echo "Updating DNS record: $FQDN -> $CONTENT (Proxied: $PROXIED)"

            if [ "$RECORD_ID" != "null" ] && [ -n "$RECORD_ID" ]; then
              curl -s -X PUT "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID" \
                -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
                -H "Content-Type: application/json" \
                --data "{\"type\":\"CNAME\",\"name\":\"$FQDN\",\"content\":\"$CONTENT\",\"proxied\":$PROXIED,\"ttl\":1}" >/dev/null
            else
              curl -s -X POST "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records" \
                -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
                -H "Content-Type: application/json" \
                --data "{\"type\":\"CNAME\",\"name\":\"$FQDN\",\"content\":\"$CONTENT\",\"proxied\":$PROXIED,\"ttl\":1}" >/dev/null
            fi
          }

          # Update Records
          update_dns_record "app" "$TUNNEL_CNAME" true
          update_dns_record "api" "$TUNNEL_CNAME" true
          update_dns_record "@" "$TUNNEL_CNAME" true

      - name: Check Health Endpoint
        run: |
          set -e

          echo "Waiting for DNS propagation and service health..."
          start_ts=$(date +%s)

          # Retry loop for 2 minutes
          for i in {1..24}; do
            http_status=$(curl -s -o /dev/null -w "%{http_code}" https://app.cloudtolocalllm.online/health || echo "000")
            if [ "$http_status" = "200" ]; then
              elapsed=$(( $(date +%s) - start_ts ))
              echo "Health check passed with HTTP 200 after ${elapsed}s!"
              exit 0
            fi

            echo "Attempt $i/24: Health check returned HTTP $http_status, waiting 5s..."
            sleep 5
          done

          echo "Health check failed after 24 attempts (~2 minutes). Collecting diagnostics..."
          kubectl get pods -n cloudtolocalllm || true
          echo "--- postgres logs (tail 50) ---"
          kubectl logs postgres-0 -n cloudtolocalllm --tail=50 || true
          kubectl describe pod postgres-0 -n cloudtolocalllm || true
          echo "--- api-backend logs (tail 100) ---"
          kubectl logs -l app=api-backend -n cloudtolocalllm --tail=100 || true
          kubectl describe pod -l app=api-backend -n cloudtolocalllm || true
          exit 1

      - name: Deployment Summary
        if: success()
        run: |
          echo "Deployment to Azure AKS completed successfully."
          echo "Cluster: ${{ env.AZURE_CLUSTER_NAME }} (resource group: ${{ env.AZURE_RESOURCE_GROUP }})"
          echo "Namespace: cloudtolocalllm"
          echo "Images pulled from registry: ${{ env.DOCKER_REGISTRY }}"
          echo "Public endpoints:"
          echo "  - App:  https://app.cloudtolocalllm.online/"
          echo "  - API:  https://api.cloudtolocalllm.online/"
